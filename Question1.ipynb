{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p03hoMY_KC5B"
   },
   "source": [
    "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
    "<h4 align=\"center\">Dr. Sajjad Amini</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2023</h4>\n",
    "\n",
    "**Student Name**:\n",
    "\n",
    "**Student ID**:\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "In this exercise, we want to examine **linear regression**. For this purpose, we have prepared a dataset in the `q1.csv` file. This dataset is used to estimate the **heating load** and **cooling load** of a building based on its parameters. The parameters in this dataset are explained below:\n",
    "\n",
    "- $X_1$: Relative Compactness\n",
    "- $X_2$: Surface Area\n",
    "- $X_3$: Wall Area\n",
    "- $X_4$: Roof Area\n",
    "- $X_5$: Overall Height\n",
    "- $X_6$: Orientation\n",
    "- $X_7$: Glazing Area\n",
    "- $X_8$: Glazing Area Distribution\n",
    "- $Y_1$: Heating Load\n",
    "- $Y_2$: Cooling Load\n",
    "\n",
    "**Note**: For the sake of simplicity, we will only focus on estimating the **heating load** in this problem. Also, please note that we have some inline questions in this notebook, for which you should write your answers in the **Answer** section below each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6_GSec1OXRM"
   },
   "source": [
    "## Importing Libraries\n",
    "\n",
    "First we import libraries that we need for this assignment.\n",
    "\n",
    "**Attention**: You should only use these libraries. Other libraries are not acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5u7kqX0wONrr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9he9c_YvMqAL"
   },
   "source": [
    "## Reading Data and Preprocessing\n",
    "\n",
    "In this section, we want to read data from a CSV file and then preprocess it to make it ready for the rest of the problem.\n",
    "\n",
    "First, we read the data in the cell below and extract an $m \\times n$ matrix, $X$, and an $m \\times 1$ vector, $Y$, from it, which represent our knowledge about the building (`X1`, `X2`, ..., `X8`) and heating load (`Y1`), respectively. Note that by $m$, we mean the number of data points and by $n$, we mean the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bV10SRSaJ_DJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = None, None\n",
    "\n",
    "### START CODE HERE ###\n",
    "data_path = \"q1.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "X = df.iloc[0:,0:8].to_numpy()\n",
    "Y = df['Y1'].to_numpy()\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkZeww-2OAuX"
   },
   "source": [
    "Next, we should normalize our data. For normalizing a vector $\\mathbf{x}$, a very common method is to use this formula:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{norm} = \\dfrac{\\mathbf{x} - \\overline{\\mathbf{x}}}{\\sigma_\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "Here, $\\overline{x}$ and $\\sigma_\\mathbf{x}$ denote the mean and standard deviation of vector $\\mathbf{x}$, respectively. Use this formula and store the new $X$ and $Y$ vectors in the cell below.\n",
    "\n",
    "**Question**: Briefly explain why we need to normalize our data before starting the training.\n",
    "\n",
    "**Answer**: By normalizing, convergence would be easier and our training will be less sesitive to the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y2i0bjxUPak2"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "m,n = X.shape\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X = (X-X_mean) / (np.linalg.norm(X-X_mean, axis=0)/np.sqrt(m))\n",
    "\n",
    "Y_mean = np.mean(Y,axis=0)\n",
    "Y = (Y-Y_mean)/(np.linalg.norm(Y-Y_mean)/np.sqrt(m))\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvmHQ-mxQzDE"
   },
   "source": [
    "Finally, we should add a column of $1$s at the beginning of $X$ to represent the bias term. Do this in the next cell. Note that after this process, $X$ should be an $m \\times (n+1)$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QLfV7VQNRCfF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "m, n = X.shape\n",
    "X = np.hstack((np.ones((m, 1)), X))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO--dppbRsdb"
   },
   "source": [
    "## Training Model Using Direct Method\n",
    "\n",
    "We know that the loss function in linear regression is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\mathbf{w}^\\top\\mathbf{x}_i-y_i)^2\n",
    "$$\n",
    "\n",
    "Here, $w$ is the weight vector and $(x_i, y_i)$ represents the $i$th data point. First, write a function that takes $X$, $Y$, and $w$ as inputs and returns the loss value in the next cell. Note that your implementation should be fully vectorized, meaning that you are not allowed to use any loops in your function and should only use functions prepared in the numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bSbXtyXCRzD3"
   },
   "outputs": [],
   "source": [
    "def loss(X, Y, w):\n",
    "  '''\n",
    "  X: an m by (n+1) matrix which includes inputs\n",
    "  Y: an m by 1 vector which includes heating loads\n",
    "  w: an (n+1) by 1 weight vector\n",
    "  '''\n",
    "  m, n = X.shape\n",
    "  \n",
    "  ### START CODE HERE ###\n",
    "  loss = 1/m*(np.linalg.norm(X @ w - Y))**2\n",
    "  ### END CODE HERE ###\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuhSB9zaTfwm"
   },
   "source": [
    "Now, we want to calculate the weight matrix, $w$, using the direct method. By direct method, we mean finding the answer to the optimization problem below directly using linear algebra, without using iterative methods:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\mathcal{L}(w)\n",
    "$$\n",
    "\n",
    "Question: What is the answer to this problem in terms of $X$ and $Y$?\n",
    "\n",
    "Answer:\n",
    "\n",
    "It is a minimum square problem. we reformulate it:\n",
    "$$\n",
    "\\min_{w} ||X w - Y||^2\n",
    "$$\n",
    "we solve it by :\n",
    "$\n",
    "w^* = (X^T X)^{-1}X^T Y\n",
    "$\n",
    "\n",
    "Now you should implement a function that receives $X$ and $Y$ as input and returns $w$. Note that your implementation should also be fully vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-gCUK01DT-cW"
   },
   "outputs": [],
   "source": [
    "def direct_method(X, Y):\n",
    "  '''\n",
    "  X: an m by (n+1) matrix which includes inputs\n",
    "  Y: an m by 1 vector which includes heating loads\n",
    "  '''\n",
    "  w = None\n",
    "  ### START CODE HERE ###\n",
    "  w = (np.linalg.pinv((X.T @ X))@X.T@Y)\n",
    "  ### END CODE HERE ###\n",
    "  return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thFIeOaSUvlw"
   },
   "source": [
    "Finally, we want to evaluate our loss for this problem. Run the cell below to calculate the loss of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6gGDh11VU8vF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [ 2.42861287e-16 -6.79038297e-01 -3.73577118e-01  7.05448253e-02\n",
      " -3.98359602e-01  7.23687495e-01 -2.58653392e-03  2.63170601e-01\n",
      "  3.13216718e-02]\n",
      "loss for this problem using direct method is 0.08379791780841636\n"
     ]
    }
   ],
   "source": [
    "w = direct_method(X, Y) # calculating w using direct method\n",
    "print(\"w = {}\".format(w))\n",
    "print(f\"loss for this problem using direct method is {loss(X, Y, w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaG14YG9VXiS"
   },
   "source": [
    "## Training Model Using Gradient Descent\n",
    "\n",
    "Now, instead of using the direct method to calculate $w$, we want to use the **Gradient Descent** algorithm. We know that in this algorithm, in each iteration, we should update our weight vector with:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha \\nabla \\mathcal{L}(\\mathbf{w}^{(t)})\n",
    "$$\n",
    "\n",
    "Here, $w^{t}$ represents the weight matrix in the $t$th iteration, and $\\alpha$ represents the learning rate.\n",
    "\n",
    "**Question**: Write an expression for $\\nabla\\mathcal{L}(\\mathbf{w})$.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "$$\n",
    "\\nabla ||X w - Y||^2 = \\nabla (X w - Y)^T(X w - Y) = \\nabla (w^TX^TXw + Y^TY + 2Y^TXw) \n",
    "$$\n",
    "by vecctor differentiation identities we have:\n",
    "$$\n",
    "\\nabla (w^T X^T X w) = (X^T X + (X^T X)^T) w = 2 X^T X w\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla  (2Y^T X w) = 2 Y^T X\n",
    "$$\n",
    "so we have\n",
    "$$\n",
    "\\nabla L(w) = \\frac{1}{m}(2 X^T X w - 2 X^T Y)\n",
    "$$\n",
    "Now, write a function that computes the gradient of $\\mathcal{L}(\\mathbf{w})$. This function should receive $X$, $Y$, and $\\mathbf{w}$ as inputs and return an $(n+1) \\times 1$ vector, which represents $\\nabla\\mathcal{L}(\\mathbf{w})$. Note that your implementation should also be **fully vectorized**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yqVQ-8I-VeVc"
   },
   "outputs": [],
   "source": [
    "def gradient(X, Y, w):\n",
    "  '''\n",
    "  X: an m by (n+1) matrix which includes inputs\n",
    "  Y: an m by 1 vector which includes heating loads\n",
    "  w: an (n+1) by 1 weight vector\n",
    "  '''\n",
    "  m, n = X.shape\n",
    "  grad = None\n",
    "  ### START CODE HERE ###\n",
    "  \n",
    "  grad = (2 * X.T @ X @ w - 2 * (X.T@Y))/m\n",
    "  ### END CODE HERE ###\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJP5KaZzXz5K"
   },
   "source": [
    "Now, we are ready to implement the Gradient Descent algorithm. Complete the function below for this purpose. Note that this function receives $X$, $Y$, the learning rate, and the number of iterations as inputs. This function should return two parameters. The first parameter is $\\mathbf{w}$, and the second parameter is a `numpy` array that contains the loss in each iteration. This array is indicated by `loss_history` in the code. Also note that you should initialize $\\mathbf{w}$ with the `randn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qdfNjz5DYgD7"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, alpha, num_iter):\n",
    "  '''\n",
    "  X: an m by (n+1) matrix which includes inputs\n",
    "  Y: an m by 1 vector which includes heating loads\n",
    "  alpha: learning rate\n",
    "  num_iter: number of iterations of the algorithm\n",
    "  '''\n",
    "  m, n = X.shape\n",
    "  w, loss_history = None,None\n",
    "  ### START CODE HERE ###\n",
    "  w = np.random.randn(n)\n",
    "\n",
    "  loss_history = []\n",
    "  for i in range(num_iter):\n",
    "    w = w - alpha * gradient(X,Y,w)\n",
    "    \n",
    "\n",
    "    loss_history.append(loss(X,Y,w))\n",
    "    \n",
    "  ### END CODE HERE ###\n",
    "  return w, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjGioRweZK9O"
   },
   "source": [
    "Now, run the `gradient_descent` function for 5 different values of the learning rate. Plot the `loss_history` of these 5 different values in the same figure.\n",
    "\n",
    "**Question**: Discuss the effect of the learning rate and find the best value of this parameter.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yMuwbOokZtcL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with alpha = 0.01 :0.08379791788328707\n",
      "w = [-7.99118547e-16 -6.78951440e-01 -9.74481110e-01  3.68198777e-01\n",
      "  2.17990097e-01  7.23725491e-01 -2.58653392e-03  2.63170601e-01\n",
      "  3.13216718e-02]\n",
      "loss with alpha = 0.02 :0.0837979178084164\n",
      "loss with alpha = 0.05 :0.08379791780841636\n",
      "loss with alpha = 0.1 :0.0837979178084164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-d7b686e70fb7>:11: RuntimeWarning: overflow encountered in matmul\n",
      "  grad = (2 * X.T @ X @ w - 2 * (X.T@Y))/m\n",
      "<ipython-input-5-2246cf302d29>:10: RuntimeWarning: invalid value encountered in matmul\n",
      "  loss = 1/m*(np.linalg.norm(X @ w - Y))**2\n",
      "<ipython-input-8-d7b686e70fb7>:11: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad = (2 * X.T @ X @ w - 2 * (X.T@Y))/m\n",
      "<ipython-input-9-0545a20937a2>:15: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - alpha * gradient(X,Y,w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with alpha = 0.5 :nan\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "n = 100000\n",
    "alpha = 0.01\n",
    "w,loss_history = gradient_descent(X,Y,alpha,n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "print(\"w = {}\".format(w))\n",
    "alpha = 0.02\n",
    "w,loss_history = gradient_descent(X,Y,alpha,n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "alpha = 0.05\n",
    "w,loss_history = gradient_descent(X,Y,alpha,n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "alpha = 0.1\n",
    "w,loss_history = gradient_descent(X,Y,alpha,n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "alpha = 0.5\n",
    "w,loss_history = gradient_descent(X,Y,alpha,n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-lyJhZqZ18d"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Compare the answer of two different methods that we used earlier.\n",
    "\n",
    "**Question**: Discuss these two methods and compare them with each other. When is it better to use the direct method, and when is it better to use Gradient Descent?\n",
    "\n",
    "**Answer**: direct method can be used when we can minimize loss function anylytically.but in some cases we can not therefore we use numerical methods like gradient descent. The problem with gradient descent is that learning rate is so important. if it is chosen very high it will not converge and if it is chosen very low it will converge too late."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MF15dsmXaUzJ"
   },
   "source": [
    "## (Additional Part) Stochastic Gradient Descent\n",
    "\n",
    "When the number of data points becomes large, calculating the gradient becomes very complicated. In these circumstances, we use **Stochastic Gradient Descent**. In this algorithm, instead of using all of the data points to calculate the gradient, we use only a small number of them. We choose these small number of points randomly in each iteration. Implement this algorithm, and use it to calculate $w$, and then compare the result with the preceding parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6cHTQFgOaQBB"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, Y, k, alpha, num_iter):\n",
    "  '''\n",
    "  X: an m by (n+1) matrix which includes inputs\n",
    "  Y: an m by 1 vector which includes heating loads\n",
    "  k: number of data points used in each iteration\n",
    "  alpha: learning rate\n",
    "  num_iter: number of iterations of the algorithm\n",
    "  '''\n",
    "  m, n = X.shape\n",
    "  w, loss_history = None, None \n",
    "  ### START CODE HERE ###\n",
    "  w = np.random.randn(n)\n",
    "\n",
    "  loss_history = []\n",
    "  for i in range(num_iter):\n",
    "    indexes = np.random.randint(m, size=k)\n",
    "    w = w - alpha * gradient(X[indexes,:],Y[indexes],w)\n",
    "    \n",
    "\n",
    "    loss_history.append(loss(X,Y,w))\n",
    "    \n",
    "  ### END CODE HERE ###\n",
    "  return w, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with alpha = 0.01 :0.08391608450364771\n",
      "loss with alpha = 0.02 :0.08429804277478103\n",
      "loss with alpha = 0.05 :0.0838163788071243\n",
      "loss with alpha = 0.2 :0.08740872704301916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-d7b686e70fb7>:11: RuntimeWarning: overflow encountered in matmul\n",
      "  grad = (2 * X.T @ X @ w - 2 * (X.T@Y))/m\n",
      "<ipython-input-5-2246cf302d29>:10: RuntimeWarning: invalid value encountered in matmul\n",
      "  loss = 1/m*(np.linalg.norm(X @ w - Y))**2\n",
      "<ipython-input-8-d7b686e70fb7>:11: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad = (2 * X.T @ X @ w - 2 * (X.T@Y))/m\n",
      "<ipython-input-11-b729e491b105>:17: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - alpha * gradient(X[indexes,:],Y[indexes],w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with alpha = 0.5 :nan\n"
     ]
    }
   ],
   "source": [
    "n = 100000\n",
    "k = 50\n",
    "alpha = 0.01\n",
    "w,loss_history = stochastic_gradient_descent(X, Y, k, alpha, n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "alpha = 0.02\n",
    "w,loss_history = stochastic_gradient_descent(X, Y, k, alpha, n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "alpha = 0.05\n",
    "w,loss_history = stochastic_gradient_descent(X, Y, k, alpha, n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "alpha = 0.2\n",
    "w,loss_history = stochastic_gradient_descent(X, Y, k, alpha, n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n",
    "\n",
    "alpha = 0.5\n",
    "w,loss_history = stochastic_gradient_descent(X, Y, k, alpha, n)\n",
    "print(\"loss with alpha = {} :{}\".format(alpha,loss_history[n-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is shown that the performance of normal gradient descent is slightly better however stochastic is faster."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
